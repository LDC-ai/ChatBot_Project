{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Expression Language(LCEL)\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/expression_language/why/\n",
    "\n",
    "### 기본 구조: 프롬프트 + 모델 + 출력 파서\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LANGCHAIN_PROJECT]\n",
      "LCEL\n"
     ]
    }
   ],
   "source": [
    "#API KEY 저장을 위한 os 라이브러리 호출\n",
    "import os\n",
    "\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'LCEL'\n",
    "print(f\"[LANGCHAIN_PROJECT]\\n{os.environ['LANGCHAIN_PROJECT']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langchain_teddynote\n",
      "  Using cached langchain_teddynote-0.3.6-py3-none-any.whl (38 kB)\n",
      "Collecting pdf2image\n",
      "  Using cached pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Collecting rank-bm25\n",
      "  Using cached rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Collecting olefile\n",
      "  Using cached olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
      "Collecting langchain\n",
      "  Using cached langchain-0.3.3-py3-none-any.whl (1.0 MB)\n",
      "Collecting openai\n",
      "  Using cached openai-1.51.2-py3-none-any.whl (383 kB)\n",
      "Collecting konlpy\n",
      "  Using cached konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
      "Collecting pinecone-client[grpc]\n",
      "  Using cached pinecone_client-5.0.1-py3-none-any.whl (244 kB)\n",
      "Collecting deepl\n",
      "  Using cached deepl-1.19.1-py3-none-any.whl (35 kB)\n",
      "Collecting langgraph\n",
      "  Using cached langgraph-0.2.35-py3-none-any.whl (108 kB)\n",
      "Collecting kiwipiepy\n",
      "  Using cached kiwipiepy-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5 MB)\n",
      "Collecting pinecone-text\n",
      "  Using cached pinecone_text-0.9.0-py3-none-any.whl (23 kB)\n",
      "Collecting feedparser\n",
      "  Using cached feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "Collecting requests<3,>=2\n",
      "  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Collecting sgmllib3k\n",
      "  Using cached sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting numpy\n",
      "  Using cached numpy-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
      "Collecting kiwipiepy-model<0.20,>=0.19\n",
      "  Using cached kiwipiepy_model-0.19.0.tar.gz (34.7 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Collecting JPype1>=0.7.0\n",
      "  Using cached JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n",
      "Collecting lxml>=4.1.0\n",
      "  Using cached lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "Collecting PyYAML>=5.3\n",
      "  Using cached PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Using cached aiohttp-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Collecting langchain-core<0.4.0,>=0.3.10\n",
      "  Using cached langchain_core-0.3.10-py3-none-any.whl (404 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4\n",
      "  Using cached SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17\n",
      "  Using cached langsmith-0.1.134-py3-none-any.whl (295 kB)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0\n",
      "  Using cached tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0\n",
      "  Using cached langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
      "Collecting langgraph-checkpoint<3.0.0,>=2.0.0\n",
      "  Using cached langgraph_checkpoint-2.0.1-py3-none-any.whl (22 kB)\n",
      "Collecting jiter<1,>=0.4.0\n",
      "  Using cached jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./.local/lib/python3.10/site-packages (from openai->langchain_teddynote) (4.12.2)\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting sniffio\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting anyio<5,>=3.5.0\n",
      "  Using cached anyio-4.6.2-py3-none-any.whl (89 kB)\n",
      "Collecting pillow\n",
      "  Using cached pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "Collecting pinecone-plugin-inference<2.0.0,>=1.0.3\n",
      "  Using cached pinecone_plugin_inference-1.1.0-py3-none-any.whl (85 kB)\n",
      "Collecting certifi>=2019.11.17\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Collecting urllib3>=1.26.0\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7\n",
      "  Using cached pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
      "Collecting googleapis-common-protos>=1.53.0\n",
      "  Using cached googleapis_common_protos-1.65.0-py2.py3-none-any.whl (220 kB)\n",
      "Collecting protobuf<5.0,>=4.25\n",
      "  Using cached protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Collecting grpcio>=1.44.0\n",
      "  Using cached grpcio-1.66.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
      "Collecting protoc-gen-openapiv2<0.0.2,>=0.0.1\n",
      "  Using cached protoc_gen_openapiv2-0.0.1-py3-none-any.whl (7.9 kB)\n",
      "Collecting lz4>=3.1.3\n",
      "  Using cached lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting types-requests<3.0.0,>=2.25.0\n",
      "  Using cached types_requests-2.32.0.20240914-py3-none-any.whl (15 kB)\n",
      "Collecting nltk<4.0.0,>=3.6.5\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in ./.local/lib/python3.10/site-packages (from pinecone-text->langchain_teddynote) (1.0.1)\n",
      "Collecting wget<4.0,>=3.2\n",
      "  Using cached wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting mmh3<5.0.0,>=4.1.0\n",
      "  Using cached mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
      "Collecting yarl<2.0,>=1.12.0\n",
      "  Using cached yarl-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (310 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0\n",
      "  Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "Collecting idna>=2.8\n",
      "  Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai->langchain_teddynote) (1.2.2)\n",
      "Collecting httpcore==1.*\n",
      "  Using cached httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
      "Collecting h11<0.15,>=0.13\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: packaging in ./.local/lib/python3.10/site-packages (from JPype1>=0.7.0->konlpy->langchain_teddynote) (24.1)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting msgpack<2.0.0,>=1.1.0\n",
      "  Using cached msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14\n",
      "  Using cached orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Collecting regex>=2021.8.3\n",
      "  Using cached regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\n",
      "Collecting click\n",
      "  Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Collecting pydantic-core==2.23.4\n",
      "  Downloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.6.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting greenlet!=0.4.17\n",
      "  Downloading greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (599 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.5/599.5 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jsonpointer>=1.9\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.9/208.9 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: kiwipiepy-model, wget, sgmllib3k\n",
      "  Building wheel for kiwipiepy-model (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kiwipiepy-model: filename=kiwipiepy_model-0.19.0-py3-none-any.whl size=34793893 sha256=1768f50e0d7639a43bde13a83e25d93650df326d57c5759043ef429cc835021a\n",
      "  Stored in directory: /home/elicer/.cache/pip/wheels/7a/1b/f4/91a56e5f2e34c0f981058fca3dca6bb7f02ff20d69d8b53c3b\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9657 sha256=054c7ac2cd35d5e1449ccd9d0b0c1efe3c875d279a54003e2a2135bd0899ce8e\n",
      "  Stored in directory: /home/elicer/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6049 sha256=5e6e4a5ecd03470c9dfd413ac976c1e4f20eb4e3f259f71d705e3041230d1c54\n",
      "  Stored in directory: /home/elicer/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
      "Successfully built kiwipiepy-model wget sgmllib3k\n",
      "Installing collected packages: wget, sgmllib3k, mmh3, kiwipiepy-model, urllib3, tqdm, tenacity, sniffio, regex, PyYAML, pydantic-core, protobuf, propcache, pinecone-plugin-interface, pillow, orjson, olefile, numpy, multidict, msgpack, lz4, lxml, jsonpointer, JPype1, joblib, jiter, idna, h11, grpcio, greenlet, frozenlist, feedparser, distro, click, charset-normalizer, certifi, attrs, async-timeout, annotated-types, aiohappyeyeballs, yarl, types-requests, SQLAlchemy, requests, rank-bm25, pydantic, pinecone-plugin-inference, pdf2image, nltk, konlpy, kiwipiepy, jsonpatch, httpcore, googleapis-common-protos, anyio, aiosignal, requests-toolbelt, protoc-gen-openapiv2, pinecone-text, pinecone-client, httpx, deepl, aiohttp, openai, langsmith, langchain-core, langgraph-checkpoint, langchain-text-splitters, langgraph, langchain, langchain_teddynote\n",
      "Successfully installed JPype1-1.5.0 PyYAML-6.0.2 SQLAlchemy-2.0.35 aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 annotated-types-0.7.0 anyio-4.6.2 async-timeout-4.0.3 attrs-24.2.0 certifi-2024.8.30 charset-normalizer-3.4.0 click-8.1.7 deepl-1.19.1 distro-1.9.0 feedparser-6.0.11 frozenlist-1.4.1 googleapis-common-protos-1.65.0 greenlet-3.1.1 grpcio-1.66.2 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 idna-3.10 jiter-0.6.1 joblib-1.4.2 jsonpatch-1.33 jsonpointer-3.0.0 kiwipiepy-0.19.0 kiwipiepy-model-0.19.0 konlpy-0.6.0 langchain-0.3.3 langchain-core-0.3.10 langchain-text-splitters-0.3.0 langchain_teddynote-0.3.6 langgraph-0.2.35 langgraph-checkpoint-2.0.1 langsmith-0.1.134 lxml-5.3.0 lz4-4.3.3 mmh3-4.1.0 msgpack-1.1.0 multidict-6.1.0 nltk-3.9.1 numpy-1.26.4 olefile-0.47 openai-1.51.2 orjson-3.10.7 pdf2image-1.17.0 pillow-10.4.0 pinecone-client-5.0.1 pinecone-plugin-inference-1.1.0 pinecone-plugin-interface-0.0.7 pinecone-text-0.9.0 propcache-0.2.0 protobuf-4.25.5 protoc-gen-openapiv2-0.0.1 pydantic-2.9.2 pydantic-core-2.23.4 rank-bm25-0.2.2 regex-2024.9.11 requests-2.32.3 requests-toolbelt-1.0.0 sgmllib3k-1.0.0 sniffio-1.3.1 tenacity-8.5.0 tqdm-4.66.5 types-requests-2.32.0.20240914 urllib3-2.2.3 wget-3.2 yarl-1.15.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_teddynote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "LCEL\n"
     ]
    }
   ],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install -qU langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"LCEL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 프롬프트 템플릿의 활용\n",
    "\n",
    "`PromptTemplate`\n",
    "\n",
    "- 사용자의 입력 변수를 사용하여 완전한 프롬프트 문자열을 만드는 데 사용되는 템플릿\n",
    "  - `template`: 템플릿 문자열. 문자열 내에서 중괄호 `{}`는 변수를 나타냄\n",
    "  - `input_variables`: 중괄호 안에 들어갈 변수의 이름을 리스트로 정의함\n",
    "\n",
    "`input_variables`\n",
    "\n",
    "- input_variables는 PromptTemplate에서 사용되는 변수의 이름을 정의하는 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import stream_response  # 스트리밍 출력\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain in ./.local/lib/python3.10/site-packages (0.3.3)\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.2.2-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in ./.local/lib/python3.10/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in ./.local/lib/python3.10/site-packages (from langchain) (0.1.134)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.10 in ./.local/lib/python3.10/site-packages (from langchain) (0.3.10)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in ./.local/lib/python3.10/site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./.local/lib/python3.10/site-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./.local/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./.local/lib/python3.10/site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.local/lib/python3.10/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in ./.local/lib/python3.10/site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in ./.local/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./.local/lib/python3.10/site-packages (from langchain) (3.10.10)\n",
      "Collecting tiktoken<1,>=0.7\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: openai<2.0.0,>=1.40.0 in ./.local/lib/python3.10/site-packages (from langchain-openai) (1.51.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.15.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.local/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (24.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.local/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.local/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (4.12.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.local/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.local/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.local/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (4.6.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (0.6.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (4.66.5)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: sniffio in ./.local/lib/python3.10/site-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./.local/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.local/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.local/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.local/lib/python3.10/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai) (1.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in ./.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.10->langchain) (3.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.local/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Installing collected packages: tiktoken, langchain-openai\n",
      "Successfully installed langchain-openai-0.2.2 tiktoken-0.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`from_template()` 메소드를 사용하여 PromptTemplate 객체 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='{country}의 수도는 어디인가요?')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# template 정의\n",
    "template = \"{country}의 수도는 어디인가요?\"\n",
    "\n",
    "# from_template 메소드를 이용하여 PromptTemplate 객체 생성\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'대한민국의 수도는 어디인가요?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt 생성\n",
    "prompt = prompt_template.format(country=\"대한민국\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'미국의 수도는 어디인가요?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt 생성\n",
    "prompt = prompt_template.format(country=\"미국\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    max_tokens=2048,\n",
    "    temperature=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain 생성\n",
    "\n",
    "#### LCEL(LangChain Expression Language)\n",
    "\n",
    "\n",
    "```\n",
    "chain = prompt | model | output_parser\n",
    "```\n",
    "\n",
    "이 체인에서 사용자 입력은 프롬프트 템플릿으로 전달되고, 그런 다음 프롬프트 템플릿 출력은 모델로 전달\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='{topic} 에 대해 쉽게 설명해주세요.')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fcf13b7eec0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fcf13b91000>, root_client=<openai.OpenAI object at 0x7fcf13b7cfa0>, root_async_client=<openai.AsyncOpenAI object at 0x7fcf13b7ef20>, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt 를 PromptTemplate 객체로 생성합니다.\n",
    "prompt = PromptTemplate.from_template(\"{topic} 에 대해 쉽게 설명해주세요.\")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain = prompt | model\n",
    "chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### invoke() 호출\n",
    "\n",
    "- python 딕셔너리 형태(키: 값)로 입력값을 전달\n",
    "- invoke() 함수 호출 시, 입력값을 전달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input 딕셔너리에 주제 설정\n",
    "input = {\"topic\": \"인공지능의 학습 방법\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='인공지능의 학습 방법은 크게 지도 학습, 비지도 학습, 강화 학습으로 나눌 수 있습니다.\\n\\n1. 지도 학습: 이는 가장 일반적인 학습 방법으로, 입력 데이터와 정답 데이터를 모두 제공하여 모델을 학습시키는 방법입니다. 예를 들어, 고양이 사진과 해당 고양이의 라벨(고양이)을 함께 제공하여 모델이 이미지를 분류할 수 있도록 학습시키는 것이 지도 학습의 한 예입니다.\\n\\n2. 비지도 학습: 이는 정답 데이터 없이 입력 데이터만을 이용하여 모델을 학습시키는 방법입니다. 모델은 데이터의 패턴이나 구조를 스스로 학습하여 클러스터링, 차원 축소, 이상 탐지 등의 작업을 수행할 수 있습니다.\\n\\n3. 강화 학습: 이는 보상을 통해 모델을 학습시키는 방법으로, 모델이 특정 행동을 취했을 때 보상을 받거나 처벌을 받으면서 학습합니다. 예를 들어, 게임에서 승리하면 보상을 받고 패배하면 처벌을 받는 방식으로 모델을 학습시키는 것이 강화 학습의 한 예입니다.\\n\\n이러한 다양한 학습 방법을 조합하여 인공지능 모델을 효과적으로 학습시킬 수 있습니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 436, 'prompt_tokens': 30, 'total_tokens': 466, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-652e435a-f4e2-4278-9c55-477dc23e51ed-0', usage_metadata={'input_tokens': 30, 'output_tokens': 436, 'total_tokens': 466, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt 객체와 model 객체를 파이프(|) 연산자로 연결하고 invoke 메서드를 사용하여 input을 전달\n",
    "# 이를 통해 AI 모델이 생성한 메시지를 반환\n",
    "chain.invoke(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인공지능의 학습 방법은 주로 머신러닝과 딥러닝 기술을 사용합니다. 이들은 데이터를 활용하여 컴퓨터가 스스로 패턴을 학습하도록 하는 방법입니다. \n",
      "\n",
      "1. 머신러닝: 머신러닝은 데이터를 바탕으로 컴퓨터가 패턴을 학습하는 방법입니다. 이를 위해 데이터를 입력하고 모델을 훈련시켜 원하는 결과를 출력하도록 합니다. 이때 모델은 데이터의 패턴을 파악하여 예측이나 분류를 수행합니다.\n",
      "\n",
      "2. 딥러닝: 딥러닝은 인공신경망을 사용하여 복잡한 문제를 해결하는 방법입니다. 여러 층의 인공신경망을 통해 데이터의 다양한 특징을 학습하고 판단합니다. 이를 통해 컴퓨터는 사람과 유사한 학습 능력을 가질 수 있습니다.\n",
      "\n",
      "이러한 방법을 통해 인공지능은 데이터를 통해 스스로 학습하고 발전하며, 다양한 분야에서 인간을 도와주는 기술로 활용되고 있습니다."
     ]
    }
   ],
   "source": [
    "# 스트리밍 출력을 위한 요청\n",
    "answer = chain.stream(input)\n",
    "# 스트리밍 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 출력파서(Output Parser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='{topic} 에 대해 쉽게 설명해주세요.')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fcf13b7eec0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fcf13b91000>, root_client=<openai.OpenAI object at 0x7fcf13b7cfa0>, root_async_client=<openai.AsyncOpenAI object at 0x7fcf13b7ef20>, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 프롬프트, 모델, 출력 파서를 연결하여 처리 체인을 구성합니다.\n",
    "chain = prompt | model | output_parser\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'인공지능은 데이터를 입력으로 받아서 패턴을 학습하고, 이를 바탕으로 새로운 데이터를 분석하거나 결정을 내리는 시스템입니다. 이를 위해서 인공지능은 먼저 학습 데이터를 활용하여 모델을 만들고, 이 모델을 통해 데이터의 패턴을 찾아내는 과정을 거칩니다. 이렇게 찾아낸 패턴을 기반으로 인공지능은 새로운 데이터를 분석하고 예측합니다. 이러한 과정을 통해 인공지능은 스스로 데이터를 이해하고 판단할 수 있게 됩니다.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain 객체의 invoke 메서드를 사용하여 input을 전달합니다.\n",
    "input = {\"topic\": \"인공지능의 학습 원리\"}\n",
    "chain.invoke(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인공지능의 학습 원리는 주로 기계학습(머신 러닝)과 딥러닝(신경망) 기술을 이용합니다. 이러한 기술은 데이터를 분석하고 패턴을 학습하여 문제를 해결하는 방식으로 작동합니다.\n",
      "\n",
      "기계학습은 주어진 데이터를 분석하여 패턴을 찾아내고, 이를 바탕으로 모델을 학습시킵니다. 모델은 입력 데이터를 받아들이고, 이를 처리하여 원하는 결과를 출력합니다. 이 과정에서 모델은 오차를 최소화하는 방향으로 학습을 진행하며, 점차적으로 정확도를 향상시킵니다.\n",
      "\n",
      "딥러닝은 인공신경망을 사용하여 복잡한 문제를 해결하는 기술로, 기계학습의 한 분야입니다. 인공신경망은 뇌의 구조를 모방하여 설계된 모델로, 입력층, 은닉층, 출력층으로 구성되어 있습니다. 각 층은 노드로 연결되어 있고, 각 노드는 가중치와 활성화 함수를 가지고 있습니다. 학습 과정에서 이러한 가중치가 조정되어 최적의 결과를 찾아냅니다.\n",
      "\n",
      "결론적으로, 인공지능의 학습 원리는 데이터를 분석하고 패턴을 학습하여 문제를 해결하는 과정으로 요약할 수 있습니다."
     ]
    }
   ],
   "source": [
    "# 스트리밍 출력을 위한 요청\n",
    "answer = chain.stream(input)\n",
    "# 스트리밍 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 템플릿을 변경하여 적용\n",
    "\n",
    "- 아래의 프롬프트 내용을 얼마든지 **변경** 가능\n",
    "- `model_name` 역시 변경하여 테스트가 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "당신은 영어를 가르치는 10년차 영어 선생님입니다. 주어진 상황에 맞는 영어 회화를 작성해 주세요.\n",
    "양식은 [FORMAT]을 참고하여 작성해 주세요.\n",
    "\n",
    "#상황:\n",
    "{question}\n",
    "\n",
    "#FORMAT:\n",
    "- 영어 회화:\n",
    "- 한글 해석:\n",
    "\"\"\"\n",
    "\n",
    "# 프롬프트 템플릿을 이용하여 프롬프트를 생성\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# ChatOpenAI 챗모델을 초기화\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    "\n",
    "# 문자열 출력 파서를 초기화\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체인을 구성\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 영어 회화:  \n",
      "  **Waiter:** Good evening! Welcome to our restaurant. How many people are in your party?  \n",
      "  **You:** Good evening! It's just me.  \n",
      "  **Waiter:** Great! Here’s the menu. Can I get you something to drink while you look it over?  \n",
      "  **You:** Yes, I’d like a glass of water, please.  \n",
      "  **Waiter:** Sure! Are you ready to order, or do you need more time?  \n",
      "  **You:** I think I'm ready. I would like the grilled chicken with a side of vegetables.  \n",
      "  **Waiter:** Excellent choice! Would you like anything else?  \n",
      "  **You:** No, that will be all for now. Thank you!  \n",
      "  **Waiter:** You're welcome! I’ll be right back with your order.\n",
      "\n",
      "- 한글 해석:  \n",
      "  **웨이터:** 좋은 저녁입니다! 저희 식당에 오신 것을 환영합니다. 몇 분이신가요?  \n",
      "  **당신:** 좋은 저녁입니다! 저 혼자입니다.  \n",
      "  **웨이터:** 좋습니다! 여기 메뉴입니다. 메뉴를 보시면서 음료수는 무엇을 드릴까요?  \n",
      "  **당신:** 네, 물 한 잔 주세요.  \n",
      "  **웨이터:** 알겠습니다! 주문할 준비가 되셨나요, 아니면 더 필요하신가요?  \n",
      "  **당신:** 저는 준비가 된 것 같아요. 그릴에 구운 치킨과 채소 사이드를 주문할게요.  \n",
      "  **웨이터:** 훌륭한 선택입니다! 다른 것 더 필요하신가요?  \n",
      "  **당신:** 아니요, 지금은 그걸로 충분해요. 감사합니다!  \n",
      "  **웨이터:** 천만에요! 주문하신 것 곧 가져다 드릴게요.\n"
     ]
    }
   ],
   "source": [
    "# 완성된 Chain을 실행하여 답변을 얻습니다.\n",
    "print(chain.invoke({\"question\": \"저는 식당에 가서 음식을 주문하고 싶어요\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완성된 Chain을 실행하여 답변을 얻습니다.\n",
    "# 스트리밍 출력을 위한 요청\n",
    "answer = chain.stream({\"question\": \"저는 식당에 가서 음식을 주문하고 싶어요\"})\n",
    "# 스트리밍 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이번에는 question 을 '미국에서 피자 주문'으로 설정하여 실행합니다.\n",
    "# 스트리밍 출력을 위한 요청\n",
    "answer = chain.stream({\"question\": \"미국에서 피자 주문\"})\n",
    "# 스트리밍 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [실습] 템플릿을 변경하여 나만의 여행 가이드 챗봇 만들기\n",
    "\n",
    "- 위의 프롬프트를 아래 주제에 맞게 **변경** 해보기\n",
    "1. 페르소나: 10년차 여행 가이드\n",
    "2. 3일간 가성비 여행 계획을 세워주는 챗봇 생성\n",
    "3. `{question}` 에는 여행갈 나라와 도시를 사용자에게 입력받음\n",
    "4. `answer` 변수를 출력하여 챗봇의 답변 결과 확인 \n",
    "5. `Langsmith` 에 접속하여 실행 내용 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tour_guide_template = \"\"\"\n",
    "당신은 여행지의 가이드를 담당하는 10년차 여행가이드입니다. 임의의 여행지에 대한 가성비 높은 3일치 계획을 세워주세요\n",
    "양식은 [Format]을 참고하여 작성해주세요\n",
    "\n",
    "# 상황:\n",
    "{question}\n",
    "\n",
    "#Format:\n",
    "- 사용자의 질문:\n",
    "- 여행 계획:\n",
    "    -1일차:\n",
    "    -2일차:\n",
    "    -3일차:\n",
    "\"\"\"\n",
    "\n",
    "# 프롬프트 템플릿을 이용하여 프롬프트를 생성\n",
    "prompt = PromptTemplate.from_template(tour_guide_template)\n",
    "\n",
    "# ChatOpenAI 챗모델을 초기화\n",
    "model = ChatOpenAI(model_name='gpt-4o-mini')\n",
    "\n",
    "# 문자열 출력 파서를 초기화\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 체인을 구성\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 사용자의 질문: 영국 런던을 여행하고 싶은데 계획을 세워줘.\n",
      "- 여행 계획:\n",
      "    - 1일차:\n",
      "        - 아침: 브릭 레인 마켓에서 로컬 푸드로 아침 식사 (예: 베이글과 커피)\n",
      "        - 오전: 타워 브리지와 런던 타워 방문 (입장료 할인 가능 여부 확인)\n",
      "        - 점심: 타워 브리지 근처의 펍에서 전통 영국식 점심 (예: 피시 앤 칩스)\n",
      "        - 오후: 세인트 폴 대성당 외부 관람 후, 밀레니엄 브릿지를 따라 테이트 모던 미술관까지 산책\n",
      "        - 저녁: 리버사이드의 레스토랑에서 저녁 식사 (예: 버거 또는 파스타)\n",
      "        - 야경: 런던 아이에서 야경 감상 (예약 시 할인 가능)\n",
      "\n",
      "    - 2일차:\n",
      "        - 아침: 호텔 근처 카페에서 간단한 아침 식사\n",
      "        - 오전: 버킹엄 궁전에서 근위병 교대식 관람 (무료)\n",
      "        - 점심: 세인트 제임스 파크에서 피크닉 (미리 샌드위치와 음료 구매)\n",
      "        - 오후: 내셔널 갤러리 방문 (입장 무료) 후 코OVENT GARDEN 탐방\n",
      "        - 저녁: 코벤트 가든의 푸드 마켓에서 다양한 스트리트 푸드 시식\n",
      "        - 공연: 웨스트 엔드에서 뮤지컬 관람 (미리 예약 시 할인 가능)\n",
      "\n",
      "    - 3일차:\n",
      "        - 아침: 호텔 조식 또는 근처 베이커리에서 패스트리와 커피\n",
      "        - 오전: 대영 박물관 방문 (입장 무료, 기부 가능)\n",
      "        - 점심: 박물관 근처의 현지 레스토랑에서 런치 스페셜\n",
      "        - 오후: 캠든 마켓 탐방 및 쇼핑 (개성 있는 상점들 방문)\n",
      "        - 저녁: 로컬 레스토랑에서 마지막 저녁 식사 (추천: 인도 혹은 이탈리안)\n",
      "        - 야경: 템즈강 유람선 타기 (저녁 시간대에 경치 감상)\n",
      "- 사용자의 질문: 부산을 여행하고 싶어요. 가성비 높은 3일치 계획을 세워주세요.\n",
      "- 여행 계획:\n",
      "    - 1일차:\n",
      "        - 오전: 해운대 해수욕장 산책 및 조식 (해운대 인근 카페에서 간단한 아침식사)\n",
      "        - 오전 중: 동백섬 탐방 (동백섬을 걸으며 자연 경관 감상)\n",
      "        - 점심: 해운대 시장에서 지역 특산물인 회덮밥 또는 밀면 맛보기\n",
      "        - 오후: 부산 아쿠아리움 방문 (사전 예약 시 할인 가능)\n",
      "        - 저녁: 해운대 바닷가 근처의 포장마차에서 신선한 해산물 안주와 함께 저녁 식사\n",
      "        - 야경: 해운대 해수욕장에서 바다를 바라보며 야경 감상\n",
      "\n",
      "    - 2일차:\n",
      "        - 오전: 감천문화마을 탐방 (마을 내 카페에서 조식)\n",
      "        - 오전 중: 자갈치 시장 방문 (신선한 해산물 구경 및 간식)\n",
      "        - 점심: 자갈치 시장 내 해산물 식당에서 회 또는 조개구이\n",
      "        - 오후: 부산타워 방문 (부산 시내 전경 감상)\n",
      "        - 저녁: BIFF 광장에서 길거리 음식 탐방 (떡볶이, 어묵 등)\n",
      "        - 야경: 광복로 거리에서 쇼핑 및 야경 감상\n",
      "\n",
      "    - 3일차:\n",
      "        - 오전: 태종대 방문 (태종대 유람선 탑승)\n",
      "        - 점심: 태종대 인근에서 지역 특산물인 돼지국밥 맛보기\n",
      "        - 오후: 송도 해수욕장 및 송도 스카이 파크 탐방 (스카이 파크에서 바다 전망 감상)\n",
      "        - 저녁: 송도 근처의 맛집에서 저녁 식사 (추천: 송도회 센터)\n",
      "        - 마무리: 부산역 근처 카페에서 여행 마무리 및 부산의 마지막 밤을 즐기기"
     ]
    }
   ],
   "source": [
    "# 완성된 Chain을 실행하여 답변을 얻습니다.\n",
    "print(chain.invoke({\"question:영국 런던을 여행하고 싶은데 계획을 세워줘.\"}))\n",
    "# 스트리밍 출력을 위한 요청\n",
    "answer = chain.stream({\"question\": \"대한민국 부산을 여행하고 싶으니 계획을 세워줘.\"})\n",
    "# 스트리밍 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완성된 Chain을 실행하여 답변을 얻습니다.\n",
    "# 스트리밍 출력을 위한 요청\n",
    "\n",
    "# 스트리밍 출력\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [실습] 템플릿을 변경하여 나만의 요리사 챗봇 만들기\n",
    "\n",
    "- 위의 프롬프트를 아래 주제에 맞게 **변경** 해보기\n",
    "1. 페르소나: 10년차 셰프\n",
    "2. 냉장고 속 재료(여러 재료도 가능)를 입력으로 받아 요리명과 레시피 출력\n",
    "3. `{food}` 에는 여행갈 냉장고 속 재료를 사용자에게 입력받음\n",
    "4. `answer` 변수를 출력하여 챗봇의 답변 결과 확인 \n",
    "5. `Langsmith` 에 접속하여 실행 내용 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 작성\n",
    "my_cheif = \"\"\"\n",
    "당신은 숙련도 높은 10년차 셰프입니다. 냉장고 속 재료를 입력받아 그 재료를 활용한 요리명과 레시피를 출력해주세요.\n",
    "양식은 [format]을 참조하세요.\n",
    "\n",
    "#상황:\n",
    "{question}\n",
    "\n",
    "#format:\n",
    "- 요리명:\n",
    "- 레시피:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(my_cheif)\n",
    "\n",
    "model = ChatOpenAI(model_name='gpt-4o-mini')\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 요리명: 매콤한 삼겹살 파스타\n",
      "\n",
      "- 레시피:\n",
      "  1. **재료 준비**: 파스타면 200g, 삼겹살 150g, 올리브유 2큰술, 다진마늘 1큰술, 청양고추 2개, 버섯플레이크 2큰술, 후추, 소금, 파슬리가루, 슬라이스치즈 2장.\n",
      "  \n",
      "  2. **파스타 삶기**: 끓는 물에 소금을 넣고 파스타면을 넣어 포장지에 적힌 시간에 맞춰 삶습니다. 삶은 후 체에 걸러 물기를 빼고, 올리브유를 약간 넣어 섞어줍니다.\n",
      "\n",
      "  3. **삼겹살 볶기**: 팬에 올리브유를 두르고 중불에서 다진마늘을 넣어 향이 올라오도록 볶습니다. 그 후 삼겹살을 추가하고, 겉이 바삭하게 익을 때까지 볶습니다.\n",
      "\n",
      "  4. **재료 혼합**: 삼겹살이 익으면 청양고추와 버섯플레이크를 넣고, 후추와 소금으로 간을 맞춥니다. 잘 섞어 2-3분 더 볶아줍니다.\n",
      "\n",
      "  5. **파스타와 혼합**: 삶은 파스타를 팬에 넣고 모든 재료가 잘 섞이도록 볶습니다. 원한다면 추가로 소금을 더할 수 있습니다.\n",
      "\n",
      "  6. **마무리**: 불을 끄고 슬라이스치즈를 위에 올려 녹을 때까지 뚜껑을 덮어둡니다. 마지막으로 파슬리가루를 뿌려 장식합니다.\n",
      "\n",
      "  7. **서빙**: 접시에 담아 따뜻하게 서빙합니다. 매콤하고 고소한 삼겹살 파스타를 즐기세요!"
     ]
    }
   ],
   "source": [
    "answer = chain.stream({'question' :'파스타면, 삼겹살, 올리브유, 다진마늘, 청양고추, 버섯플레이크, 후추, 소금, 파슬리가루, 슬라이스치즈'})\n",
    "\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [실습] 템플릿을 변경하여 나만의 헬스 트레이너 챗봇 만들기\n",
    "\n",
    "- 위의 프롬프트를 아래 주제에 맞게 **변경** 해보기\n",
    "1. 페르소나: 10년차 헬스 트레이너\n",
    "2. 운동하고 싶은 신체 부위를 입력하면 운동 루틴을 출력 \n",
    "3. `{today}` 에는 운동하고 싶은 신체 부위를 사용자에게 입력받음\n",
    "4. `answer` 변수를 출력하여 챗봇의 답변 결과 확인 \n",
    "5. `Langsmith` 에 접속하여 실행 내용 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 작성"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

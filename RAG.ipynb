{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv('./.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터 로드 및 전처리\n",
    "max_seq_length = 2048\n",
    "dtype = None # Tesla T4와 V100의 경우에는 Float16, Ampere+ 이상의 경우에는 Bfloat16으로 설정\n",
    "load_in_4bit = True # 메모리 사용량을 줄이기 위해 4bit 양자화를 권장\n",
    "\n",
    "df = pd.read_csv('merged_data.csv')\n",
    "texts = df['context_learn'].to_pandas().tolist()\n",
    "documents = [Document(page_content=text) for text in texts]\n",
    "\n",
    "# 2. FastLanguageModel을 통한 임베딩 모델 설정\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"Qwen/Qwen2-7B-Instruct\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit\n",
    ")\n",
    "\n",
    "# 3. Hugging Face 임베딩 설정\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"Qwen/Qwen2-7B-Instruct\")\n",
    "\n",
    "# 4. 임베딩 파일 로드 또는 생성\n",
    "embedding_file = \"embeddings.pkl\"\n",
    "if os.path.exists(embedding_file):\n",
    "    with open(embedding_file, \"rb\") as f:\n",
    "        all_embeddings = pickle.load(f)\n",
    "    print(\"임베딩이 파일에서 불러와졌습니다.\")\n",
    "else:\n",
    "    # 임베딩 생성 및 저장\n",
    "    all_embeddings = [embedding.embed_text(text) for text in texts]\n",
    "    with open(embedding_file, \"wb\") as f:\n",
    "        pickle.dump(all_embeddings, f)\n",
    "\n",
    "# 5. FAISS 인덱스 생성 및 벡터 스토어 설정\n",
    "embedding_dim = len(all_embeddings[0])\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(np.array(all_embeddings).astype('float32'))\n",
    "vectorstore = FAISS(embedding_function=embedding.embed_query, index=index, docstore={i: doc for i, doc in enumerate(documents)}, index_to_docstore_id={i: i for i in range(len(documents))})\n",
    "\n",
    "# 6. LangChain Retriever 설정\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 7. 프롬프트 템플릿 생성\n",
    "prompt_template = \"\"\"\n",
    "당신은 질문-답변(Question-Answering)을 수행하는 친절한 AI 어시스턴트입니다.\n",
    "당신의 임무는 주어진 문맥(context)에서 주어진 질문(question)에 답하는 것입니다.\n",
    "검색된 다음 문맥(context)을 사용하여 질문(question)에 답하세요.\n",
    "만약, 주어진 문맥(context)에서 답을 찾을 수 없다면, 답을 모른다면\n",
    "'주어진 정보에서 질문에 대한 정보를 찾을 수 없습니다'라고 답하세요.\n",
    "한글로 답변해 주세요. 단, 기술적인 용어나 이름은 번역하지 않고 그대로 사용해 주세요.\n",
    "\n",
    "#Question:\n",
    "{question}\n",
    "\n",
    "#Context:\n",
    "{context}\n",
    "\n",
    "#Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# 8. LangChain HuggingFacePipeline을 사용하여 LLM 설정\n",
    "llm_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
    "\n",
    "# 9. RetrievalQA 체인 생성\n",
    "qa_chain = RetrievalQA(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "# 10. 질문 설정 및 질의응답 수행\n",
    "query = \"김영삼이 언급된 회의 내용을 알려줘.\"\n",
    "result = qa_chain.run({\"question\": query})\n",
    "print(\"답변:\", result)\n",
    "\n",
    "# 11. 메모리 정리\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### csv 기반 QA(Question-Answering) 챗봇으로 변경하는 코드\n",
    "\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "# CSV 파일 로드\n",
    "loader = CSVLoader(file_path=\"merged_data.csv\")\n",
    "docs = loader.load()\n",
    "print(f\"문서의 수: {len(docs)}\")\n",
    "\n",
    "# 10번째 페이지의 내용 출력\n",
    "print(f\"\\n[페이지내용]\\n{docs[0].page_content}\")\n",
    "print(f\"\\n[metadata]\\n{docs[0].metadata}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_model.embed_query(\"테스트\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n",
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:05<00:00,  1.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# 필요한 모듈 import\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "import torch\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "# CSV 파일 경로\n",
    "csv_file_path = 'merged_data.csv'\n",
    "\n",
    "# CSV 파일에서 context_learn 컬럼 로드\n",
    "df = pd.read_csv(csv_file_path)\n",
    "context_learn_texts = df['context_learn'].astype(str).tolist()\n",
    "documents = [Document(page_content=text) for text in context_learn_texts]  # Document 객체 생성\n",
    "\n",
    "# 디바이스 설정 (GPU 사용)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # GPU 사용 확인\n",
    "\n",
    "# 임베딩 모델 설정 (다국어 모델)\n",
    "embedding_model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model_name,\n",
    "    model_kwargs={'device': device}\n",
    ")\n",
    "\n",
    "# 벡터 저장소 파일 경로\n",
    "vectorstore_file = 'merged_data_faiss_index.index'\n",
    "\n",
    "# FAISS 인덱스 설정\n",
    "embed_dimension = len(embedding_model.embed_query(\"테스트\"))  # 임베딩 차원 수 자동 설정\n",
    "index = faiss.IndexFlatL2(embed_dimension)\n",
    "\n",
    "# VectorStore 설정\n",
    "if os.path.exists(vectorstore_file):\n",
    "    index = faiss.read_index(vectorstore_file)\n",
    "else:\n",
    "    embeddings = embedding_model.embed_documents(context_learn_texts)\n",
    "    embeddings = np.array(embeddings).astype('float32')  # numpy 배열로 변환\n",
    "    index.add(embeddings)  # FAISS에 임베딩 추가\n",
    "    faiss.write_index(index, vectorstore_file)  # 인덱스 파일 저장\n",
    "\n",
    "# `docstore`와 `index_to_docstore_id` 설정\n",
    "index_to_docstore_id = {i: i for i in range(len(documents))}\n",
    "docstore = InMemoryDocstore({i: doc for i, doc in enumerate(documents)})  # InMemoryDocstore로 설정\n",
    "\n",
    "vectorstore = FAISS(\n",
    "    index=index,\n",
    "    docstore=docstore,\n",
    "    index_to_docstore_id=index_to_docstore_id,\n",
    "    embedding_function=embedding_model.embed_query  # 임베딩 함수 설정\n",
    ")\n",
    "\n",
    "# Retriever 설정\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 프롬프트 템플릿 생성\n",
    "prompt_template = \"\"\"\n",
    "당신은 질문-답변(Question-Answering)을 수행하는 친절한 AI 어시스턴트입니다.\n",
    "주어진 문맥(context)을 사용하여 질문(question)에 답하세요.\n",
    "문맥에서 답을 찾을 수 없거나 모르는 경우, '주어진 정보에서 답변을 찾을 수 없습니다'라고 답하세요.\n",
    "\n",
    "# Question:\n",
    "{question}\n",
    "\n",
    "# Context:\n",
    "{context}\n",
    "\n",
    "# Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# LLM 모델 설정\n",
    "llm_model_name = \"Qwen/Qwen-7B-Chat\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(llm_model_name, device_map='auto', trust_remote_code=True)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,  # 생성할 최대 토큰 수\n",
    "    temperature=0,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# RetrievalQA 체인 생성\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유사 문서 1: 그 문의와 관련하여 추출된 내용은 다음과 같습니다.'또 따로 논의해야 되지 않나요? 그럼요. 지금 여러 가지 이슈가 있어요. 그 전에 김영식·김철민·이탄희, 저금리 전환대출 확대와 관련된 부분은 교육부에서 이탄희 의원안으로 정리해서 얘기하자라고 했는데 그 부분에 대한 논의는 없었어요. 그것은 수용한 건가요? 그렇지요. 그것을 수용했다 그러면 다음 이슈로, 지금 김영식·김철민·이탄희 안은 이탄희 안으로 정리하는 것으로 협의가 되면 김철민 의원안에서는 교육·훈련시설 이것은 빼고 그다음에 교육부가 제시한 대로 교육부장관이 정하는 교육훈련기관이라는 것을 추가로 넣어 주는 것으로 자구를 정리하면 될 거고요.' 이 부분은 김영식, 김철민, 이탄희 의원의 저금리 전환대출 확대와 관련된 안건이 교육부에서 이탄희 의원의 안으로 정리되어 협의에서 수용되었음을 보여줍니다.\n",
      "\n",
      "유사 문서 2: 박주선 부의장님과 국회의 동료 의원 여러분, 이낙연 국무총리님, 그리고 국민 여러분께 전하는 바입니다. 제가 속한 민주평화당의 첫 대정부 질문에서 말씀드리고자 하는 중요한 사항은, 평창 올림픽을 계기로 한 남북 관계 개선의 일환으로 북한 방문단이 곧 방문할 예정이라는 소식입니다. 여기에는 북한 김정은 위원장의 동생 김여정이 포함되어 있다는 보도가 있습니다. 이는 남북 관계에서 매우 높은 수준의 참여이며, 이러한 북한의 고위급 인사 방문이 남북관계의 개선과 평화 분위기 조성에 큰 도움이 될 것이라는 기대감을 갖고 있습니다. 그러나 현재로서 국무총리님께서는 이러한 사실을 공식적으로 확인해주실 수는 없으며, 공식 발표가 있을 때까지는 구체적인 내용을 확인하기 어려운 상태임을 이해하고 있습니다. 앞으로 진행될 공식 발표를 통해, 김여정 씨의 방문이 확정될 경우, 이는 국내외적으로 중대한 의미를 가지며, 한반도와 국제사회에 어떤 영향을 미칠지 주목되는 상황입니다.\n",
      "\n",
      "유사 문서 3: 경기 광명을의 양기대 위원께서 남부청장인 김원준 청장님과 김남현 북부청장님에게 감사와 격려의 말씀을 전한 후, 화천대유 대주주 김만배 씨와 이성문 전 대표 간의 수상한 자금 흐름에 대한 경찰의 수사 진행 상황에 대해 질문하셨습니다. 김원준 청장님은 수사가 초기 단계임에도 불구하고 돈의 흐름을 끝까지 추적하겠다는 자신감을 보였음에도 목적지를 아직 발견하지 못했다고 밝혔습니다. 또한, 양기대 위원은 남부청의 데이트폭력 신고 건수가 전국 최고 수준임을 언급하며, 신변보호 신청에 비해 스마트워치의 수가 부족하지 않냐고 질문했고, 남부청장은 현재 보유한 스마트워치가 562대에 더해 도경청에서 72대를 예비로 갖고 있어 충분하다고 답했습니다. 양기대 위원은 데이트폭력 신고 증가에 대응하기 위한 스마트워치 보급 확대 및 신변보호 대상 확장을 검토하고 있다고 설명하며, 이에 대해 남부청과 도경청이 수급 상황을 원활히 관리하고 있다는 점을 강조했습니다.\n",
      "\n",
      "유사 문서 4: 텍스트에서 제공된 정보에 기반하여, 한국의 검찰 총장은 청문회에서 국민의힘 소속 전주혜 위원의 질의응답을 통해 검찰 수사의 중립성과 공정성을 강조하며 자신의 수사 의지를 설명하였음을 알 수 있습니다. 전주혜 위원은 대장동 사건에 대한 관심이 고조되고 있는 상황에서, 당시 성남시장이었던 이재명 후보의 역할과 연루 여부에 대한 철저한 수사를 촉구하였고, 라임․타머스사건과 성남 지원 사건, 그리고 성남시 고문변호사 역할 등 총장의 과거 행적에 대해 질문하며, 이것이 현재 수사에 영향을 미치는지를 집중적으로 검토했습니다. 총장은 수사팀이 공정하고 중립적으로 활동하며 관련된 모든 사건에 대해 양당 및 시민단체의 고발을 받고 있어 철저한 수사가 진행 중임을 강조하였고, 그의 역할이 중립적인 수사 의지에 영향을 미치지 않음을 거듭 확언하였으며, 성남시 고문변호사 역할은 지역 봉사의 일환으로 대동고 인맥과는 무관하다고 답변하여, 검찰의 독립성을 보장하며 사건의 실체적 진실 규명을 위해 앞으로도 힘쓸 것임을 피력하였습니다.\n",
      "\n",
      "유사 문서 5: 2016년 12월 6일 국회 정무위원회에서 열린 국정감사에서 더불어민주당 김성수 의원은 현재의 국정운영에 대한 비판적인 시각을 제시하면서, 대통령과 인적쇄신에 대한 제언과 국무총리비서실장이 이를 동의하여 어떤 대응을 취할지에 대한 의견을 물어보았다. 김성수 의원은 먼저 남북관계, 국제관계, 경제 등 현안을 짚으면서, '대한민국은 온통 최고경영자들이 갖고 있는 위기 요인이 어느 때보다 크다'고 지적했다. 특히 북한의 김정은 정권이 핵을 개발하고, 우리 경제의 CEO 리스크가 심각하다고 우려했다. 또한 대통령의 국정운영 지지율이 26%에 불과한데도, 국무회의와 대수비를 묵언수행하는 것처럼 운영하는 것은 국민들이 보기에 헌법을 유린하는 행위라고 비판했다. 또한 전경련이 어버이연합에 돈을 대 줘 집회시위를 하게 하는 것은 헌법을 유린하는 행위라고 지적했다. 김성수 의원은 이러한 비판을 바탕으로, 대통령이 국정운영에 대한 전반적인 사고를 바꾸기 위해 인적쇄신이 필요하다고 제안했다. 우병우 수석 교체 수준이 아니라 청와대 참모진 전면적인 인적쇄신이 필요하다는 것이다. 또한 내각 전면 교체도 필요하다고 주장했다. 마지막으로 김성수 의원은 국무총리비서실장이 이러한 제언을 동의하여 총리에게 건의할 것인지를 물었다. 이에 국무총리비서실장은 '건의를 드리겠다'고 답했다. 김성수 의원의 이 발언은 현재의 국정운영에 대한 비판적인 시각을 대변하는 것으로 평가된다. 또한 대통령과 인적쇄신에 대한 제언은 향후 국정운영에 어떤 영향을 미칠지 주목된다.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elicer/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/elicer/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/elicer/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:612: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "답변: \n",
      "당신은 질문-답변(Question-Answering)을 수행하는 친절한 AI 어시스턴트입니다.\n",
      "주어진 문맥(context)을 사용하여 질문(question)에 답하세요.\n",
      "문맥에서 답을 찾을 수 없거나 모르는 경우, '주어진 정보에서 답변을 찾을 수 없습니다'라고 답하세요.\n",
      "\n",
      "# Question:\n",
      "김영삼이 언급된 회의에 대해 알려줘.\n",
      "\n",
      "# Context:\n",
      "그 문의와 관련하여 추출된 내용은 다음과 같습니다.'또 따로 논의해야 되지 않나요? 그럼요. 지금 여러 가지 이슈가 있어요. 그 전에 김영식·김철민·이탄희, 저금리 전환대출 확대와 관련된 부분은 교육부에서 이탄희 의원안으로 정리해서 얘기하자라고 했는데 그 부분에 대한 논의는 없었어요. 그것은 수용한 건가요? 그렇지요. 그것을 수용했다 그러면 다음 이슈로, 지금 김영식·김철민·이탄희 안은 이탄희 안으로 정리하는 것으로 협의가 되면 김철민 의원안에서는 교육·훈련시설 이것은 빼고 그다음에 교육부가 제시한 대로 교육부장관이 정하는 교육훈련기관이라는 것을 추가로 넣어 주는 것으로 자구를 정리하면 될 거고요.' 이 부분은 김영식, 김철민, 이탄희 의원의 저금리 전환대출 확대와 관련된 안건이 교육부에서 이탄희 의원의 안으로 정리되어 협의에서 수용되었음을 보여줍니다.\n",
      "\n",
      "박주선 부의장님과 국회의 동료 의원 여러분, 이낙연 국무총리님, 그리고 국민 여러분께 전하는 바입니다. 제가 속한 민주평화당의 첫 대정부 질문에서 말씀드리고자 하는 중요한 사항은, 평창 올림픽을 계기로 한 남북 관계 개선의 일환으로 북한 방문단이 곧 방문할 예정이라는 소식입니다. 여기에는 북한 김정은 위원장의 동생 김여정이 포함되어 있다는 보도가 있습니다. 이는 남북 관계에서 매우 높은 수준의 참여이며, 이러한 북한의 고위급 인사 방문이 남북관계의 개선과 평화 분위기 조성에 큰 도움이 될 것이라는 기대감을 갖고 있습니다. 그러나 현재로서 국무총리님께서는 이러한 사실을 공식적으로 확인해주실 수는 없으며, 공식 발표가 있을 때까지는 구체적인 내용을 확인하기 어려운 상태임을 이해하고 있습니다. 앞으로 진행될 공식 발표를 통해, 김여정 씨의 방문이 확정될 경우, 이는 국내외적으로 중대한 의미를 가지며, 한반도와 국제사회에 어떤 영향을 미칠지 주목되는 상황입니다.\n",
      "\n",
      "경기 광명을의 양기대 위원께서 남부청장인 김원준 청장님과 김남현 북부청장님에게 감사와 격려의 말씀을 전한 후, 화천대유 대주주 김만배 씨와 이성문 전 대표 간의 수상한 자금 흐름에 대한 경찰의 수사 진행 상황에 대해 질문하셨습니다. 김원준 청장님은 수사가 초기 단계임에도 불구하고 돈의 흐름을 끝까지 추적하겠다는 자신감을 보였음에도 목적지를 아직 발견하지 못했다고 밝혔습니다. 또한, 양기대 위원은 남부청의 데이트폭력 신고 건수가 전국 최고 수준임을 언급하며, 신변보호 신청에 비해 스마트워치의 수가 부족하지 않냐고 질문했고, 남부청장은 현재 보유한 스마트워치가 562대에 더해 도경청에서 72대를 예비로 갖고 있어 충분하다고 답했습니다. 양기대 위원은 데이트폭력 신고 증가에 대응하기 위한 스마트워치 보급 확대 및 신변보호 대상 확장을 검토하고 있다고 설명하며, 이에 대해 남부청과 도경청이 수급 상황을 원활히 관리하고 있다는 점을 강조했습니다.\n",
      "\n",
      "텍스트에서 제공된 정보에 기반하여, 한국의 검찰 총장은 청문회에서 국민의힘 소속 전주혜 위원의 질의응답을 통해 검찰 수사의 중립성과 공정성을 강조하며 자신의 수사 의지를 설명하였음을 알 수 있습니다. 전주혜 위원은 대장동 사건에 대한 관심이 고조되고 있는 상황에서, 당시 성남시장이었던 이재명 후보의 역할과 연루 여부에 대한 철저한 수사를 촉구하였고, 라임․타머스사건과 성남 지원 사건, 그리고 성남시 고문변호사 역할 등 총장의 과거 행적에 대해 질문하며, 이것이 현재 수사에 영향을 미치는지를 집중적으로 검토했습니다. 총장은 수사팀이 공정하고 중립적으로 활동하며 관련된 모든 사건에 대해 양당 및 시민단체의 고발을 받고 있어 철저한 수사가 진행 중임을 강조하였고, 그의 역할이 중립적인 수사 의지에 영향을 미치지 않음을 거듭 확언하였으며, 성남시 고문변호사 역할은 지역 봉사의 일환으로 대동고 인맥과는 무관하다고 답변하여, 검찰의 독립성을 보장하며 사건의 실체적 진실 규명을 위해 앞으로도 힘쓸 것임을 피력하였습니다.\n",
      "\n",
      "# Answer:\n",
      "검찰 총장은 청문회에서 국민의힘 소속 전주혜\n",
      "'t\n",
      "출처 문서: [Document(metadata={}, page_content=\"그 문의와 관련하여 추출된 내용은 다음과 같습니다.'또 따로 논의해야 되지 않나요? 그럼요. 지금 여러 가지 이슈가 있어요. 그 전에 김영식·김철민·이탄희, 저금리 전환대출 확대와 관련된 부분은 교육부에서 이탄희 의원안으로 정리해서 얘기하자라고 했는데 그 부분에 대한 논의는 없었어요. 그것은 수용한 건가요? 그렇지요. 그것을 수용했다 그러면 다음 이슈로, 지금 김영식·김철민·이탄희 안은 이탄희 안으로 정리하는 것으로 협의가 되면 김철민 의원안에서는 교육·훈련시설 이것은 빼고 그다음에 교육부가 제시한 대로 교육부장관이 정하는 교육훈련기관이라는 것을 추가로 넣어 주는 것으로 자구를 정리하면 될 거고요.' 이 부분은 김영식, 김철민, 이탄희 의원의 저금리 전환대출 확대와 관련된 안건이 교육부에서 이탄희 의원의 안으로 정리되어 협의에서 수용되었음을 보여줍니다.\"), Document(metadata={}, page_content='박주선 부의장님과 국회의 동료 의원 여러분, 이낙연 국무총리님, 그리고 국민 여러분께 전하는 바입니다. 제가 속한 민주평화당의 첫 대정부 질문에서 말씀드리고자 하는 중요한 사항은, 평창 올림픽을 계기로 한 남북 관계 개선의 일환으로 북한 방문단이 곧 방문할 예정이라는 소식입니다. 여기에는 북한 김정은 위원장의 동생 김여정이 포함되어 있다는 보도가 있습니다. 이는 남북 관계에서 매우 높은 수준의 참여이며, 이러한 북한의 고위급 인사 방문이 남북관계의 개선과 평화 분위기 조성에 큰 도움이 될 것이라는 기대감을 갖고 있습니다. 그러나 현재로서 국무총리님께서는 이러한 사실을 공식적으로 확인해주실 수는 없으며, 공식 발표가 있을 때까지는 구체적인 내용을 확인하기 어려운 상태임을 이해하고 있습니다. 앞으로 진행될 공식 발표를 통해, 김여정 씨의 방문이 확정될 경우, 이는 국내외적으로 중대한 의미를 가지며, 한반도와 국제사회에 어떤 영향을 미칠지 주목되는 상황입니다.'), Document(metadata={}, page_content='경기 광명을의 양기대 위원께서 남부청장인 김원준 청장님과 김남현 북부청장님에게 감사와 격려의 말씀을 전한 후, 화천대유 대주주 김만배 씨와 이성문 전 대표 간의 수상한 자금 흐름에 대한 경찰의 수사 진행 상황에 대해 질문하셨습니다. 김원준 청장님은 수사가 초기 단계임에도 불구하고 돈의 흐름을 끝까지 추적하겠다는 자신감을 보였음에도 목적지를 아직 발견하지 못했다고 밝혔습니다. 또한, 양기대 위원은 남부청의 데이트폭력 신고 건수가 전국 최고 수준임을 언급하며, 신변보호 신청에 비해 스마트워치의 수가 부족하지 않냐고 질문했고, 남부청장은 현재 보유한 스마트워치가 562대에 더해 도경청에서 72대를 예비로 갖고 있어 충분하다고 답했습니다. 양기대 위원은 데이트폭력 신고 증가에 대응하기 위한 스마트워치 보급 확대 및 신변보호 대상 확장을 검토하고 있다고 설명하며, 이에 대해 남부청과 도경청이 수급 상황을 원활히 관리하고 있다는 점을 강조했습니다.'), Document(metadata={}, page_content='텍스트에서 제공된 정보에 기반하여, 한국의 검찰 총장은 청문회에서 국민의힘 소속 전주혜 위원의 질의응답을 통해 검찰 수사의 중립성과 공정성을 강조하며 자신의 수사 의지를 설명하였음을 알 수 있습니다. 전주혜 위원은 대장동 사건에 대한 관심이 고조되고 있는 상황에서, 당시 성남시장이었던 이재명 후보의 역할과 연루 여부에 대한 철저한 수사를 촉구하였고, 라임․타머스사건과 성남 지원 사건, 그리고 성남시 고문변호사 역할 등 총장의 과거 행적에 대해 질문하며, 이것이 현재 수사에 영향을 미치는지를 집중적으로 검토했습니다. 총장은 수사팀이 공정하고 중립적으로 활동하며 관련된 모든 사건에 대해 양당 및 시민단체의 고발을 받고 있어 철저한 수사가 진행 중임을 강조하였고, 그의 역할이 중립적인 수사 의지에 영향을 미치지 않음을 거듭 확언하였으며, 성남시 고문변호사 역할은 지역 봉사의 일환으로 대동고 인맥과는 무관하다고 답변하여, 검찰의 독립성을 보장하며 사건의 실체적 진실 규명을 위해 앞으로도 힘쓸 것임을 피력하였습니다.')]\n"
     ]
    }
   ],
   "source": [
    "# 질의응답 테스트\n",
    "# 질문 설정 및 벡터화\n",
    "query = \"김영삼이 언급된 회의에 대해 알려줘.\"\n",
    "query_embedding = embedding_model.embed_query(query)  # 질문을 벡터화\n",
    "\n",
    "# FAISS 인덱스를 통해 유사 문서 검색\n",
    "k = 5  # 검색할 상위 k개의 유사 문서 수\n",
    "distances, indices = index.search(np.array([query_embedding], dtype='float32'), k)\n",
    "\n",
    "# 검색된 유사 문서 출력\n",
    "similar_docs = [documents[i] for i in indices[0]]\n",
    "for i, doc in enumerate(similar_docs):\n",
    "    print(f\"유사 문서 {i+1}: {doc.page_content}\\n\")\n",
    "\n",
    "result = qa_chain({\"query\": query})\n",
    "print(\"답변:\", result['result'])\n",
    "print(\"출처 문서:\", result['source_documents'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.schema import Document\n",
    "\n",
    "# CSV 파일 로드\n",
    "df = pd.read_csv(\"merged_data.csv\")\n",
    "\n",
    "# Document 객체 생성 및 각 열을 메타데이터로 추가\n",
    "documents = []\n",
    "for _, row in df.iterrows():\n",
    "    metadata = row.to_dict()  # 각 열을 메타데이터로 변환\n",
    "    documents.append(Document(page_content=metadata['context_learn'], metadata=metadata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

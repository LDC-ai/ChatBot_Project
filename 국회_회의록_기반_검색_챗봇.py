import os
import re
import json
import torch
import streamlit as st
from typing import TypedDict
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.tools import Tool
from langchain.agents import initialize_agent, AgentType
from langchain.memory import ConversationBufferMemory
from langchain.schema import Document
from langgraph.graph import END, StateGraph
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.runnables import RunnableConfig
from langchain.retrievers import BM25Retriever, EnsembleRetriever, MultiQueryRetriever
from langchain_openai import OpenAIEmbeddings

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="êµ­íšŒ íšŒì˜ë¡ AIê²€ìƒ‰ ì„œë¹„ìŠ¤", layout="wide", page_icon="ğŸ¤–")

with st.sidebar:
    st.session_state["OPENAI_API"] = st.text_input(label="OPENAI API í‚¤", placeholder="Enter Your API Key", value="", type="password")
    st.session_state["LANGCHAIN_API"] = st.text_input(label="LANGCHAIN API í‚¤", placeholder="Enter Your API Key", value="", type="password")

    st.markdown('---')

# OpenAI API í‚¤ê°€ ì…ë ¥ë˜ì—ˆëŠ”ì§€ í™•ì¸
if st.session_state["OPENAI_API"] and st.session_state["LANGCHAIN_API"]:
    os.environ['OPENAI_API_KEY'] = st.session_state["OPENAI_API"]
    os.environ['LANGCHAIN_API_KEY'] = st.session_state["LANGCHAIN_API"]

    # JSON íŒŒì¼ì„ ì—´ê³  ë¡œë“œ
    with open('merged_data.json', 'r', encoding='utf-8') as file:
        data = json.load(file)


    # JSON ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
    json_string = str(data)

    # íŠ¹ì • ê¸°ì¤€ ë¬¸ìì—´ì„ ì‚¬ìš©í•˜ì—¬ ë¶„ë¦¬
    split_docs = json_string.split('}}')

    # ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    organized_data_list = []

    for data_str in split_docs:
        # ê° í•„ë“œ ì¶”ì¶œ
        organized_data = {}
        
        # ê° í•„ë“œì— ëŒ€í•´ ì •ê·œ í‘œí˜„ì‹ì„ ì ìš©í•˜ê³  ê²°ê³¼ í™•ì¸
        filename_match = re.search(r"'filename': '([^']*)'", data_str)
        organized_data['filename'] = filename_match.group(1) if filename_match else None

        date_match = re.search(r"'date': '([^']*)'", data_str)
        organized_data['date'] = date_match.group(1) if date_match else None

        conference_number_match = re.search(r"'conference_number': '([^']*)'", data_str)
        organized_data['conference_number'] = conference_number_match.group(1) if conference_number_match else None

        question_number_match = re.search(r"'question_number': '([^']*)'", data_str)
        organized_data['question_number'] = question_number_match.group(1) if question_number_match else None

        meeting_name_match = re.search(r"'meeting_name': '([^']*)'", data_str)
        organized_data['meeting_name'] = meeting_name_match.group(1) if meeting_name_match else None

        generation_number_match = re.search(r"'generation_number': '([^']*)'", data_str)
        organized_data['generation_number'] = generation_number_match.group(1) if generation_number_match else None

        committee_name_match = re.search(r"'committee_name': '([^']*)'", data_str)
        organized_data['committee_name'] = committee_name_match.group(1) if committee_name_match else None

        meeting_number_match = re.search(r"'meeting_number': '([^']*)'", data_str)
        organized_data['meeting_number'] = meeting_number_match.group(1) if meeting_number_match else None

        session_number_match = re.search(r"'session_number': '([^']*)'", data_str)
        organized_data['session_number'] = session_number_match.group(1) if session_number_match else None

        agenda_match = re.search(r"'agenda': '([^']*)'", data_str)
        organized_data['agenda'] = agenda_match.group(1) if agenda_match else None

        law_match = re.search(r"'law': '([^']*)'", data_str)
        organized_data['law'] = law_match.group(1) if law_match else None

        context_match = re.search(r"'context': '([^']*)'", data_str)
        organized_data['context'] = context_match.group(1) if context_match else None

        context_learn_match = re.search(r"'context_learn': '([^']*)'", data_str)
        organized_data['context_learn'] = context_learn_match.group(1) if context_learn_match else None

        context_summary_match = re.search(r"'context_summary': {'summary_q': '([^']*)', 'summary_a': '([^']*)'}", data_str)
        organized_data['context_summary'] = context_summary_match.groups() if context_summary_match else (None, None)

        questioner_name_match = re.search(r"'questioner_name': '([^']*)'", data_str)
        organized_data['questioner_name'] = questioner_name_match.group(1) if questioner_name_match else None

        questioner_position_match = re.search(r"'questioner_position': '([^']*)'", data_str)
        organized_data['questioner_position'] = questioner_position_match.group(1) if questioner_position_match else None

        question_match = re.search(r"'question': {'tag': '[^']*', 'comment': '([^']*)', 'keyword': '([^']*)'}", data_str)
        organized_data['question'] = question_match.groups() if question_match else (None, None)

        answerer_name_match = re.search(r"'answerer_name': '([^']*)'", data_str)
        organized_data['answerer_name'] = answerer_name_match.group(1) if answerer_name_match else None

        # ìƒˆ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        organized_data_list.append(str(organized_data))

    # Langchain Document ê°ì²´ë¡œ ë³€í™˜
    documents = [Document(page_content=doc.strip() + '}}') for doc in organized_data_list if doc.strip()]

    # CUDA ì‚¬ìš© ì„¤ì •
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    # ì„ë² ë”© ëª¨ë¸ ì„¤ì • (GPU ì‚¬ìš©)
    embedding_model_name = 'jhgan/ko-sroberta-multitask'
    embedding_model = HuggingFaceEmbeddings(
        model_name=embedding_model_name,
        model_kwargs={'device': device}
    )

    # FAISS ì¸ë±ìŠ¤ ì„¤ì •
    index_path = 'json_faiss_index'

    # VectorStore ìƒì„± ë˜ëŠ” ë¡œë“œ
    if os.path.exists(index_path):
        print("ì €ì¥ëœ FAISS ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
        vectorstore = FAISS.load_local(
            index_path,
            embeddings=embedding_model,
            allow_dangerous_deserialization=True
        )
    else:
        print("FAISS ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
        vectorstore = FAISS.from_documents(documents, embedding_model)
        vectorstore.save_local(index_path)

    # LLM ì •ì˜
    llm = ChatOpenAI(model_name="gpt-4o", temperature=0.7)

    # bm25 retrieverì™€ faiss retrieverë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    bm25_retriever = BM25Retriever.from_texts(
        split_docs,
    )
    bm25_retriever.k =1

    embedding = OpenAIEmbeddings()  # OpenAI ì„ë² ë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    faiss_retriever = vectorstore.as_retriever(search_kwargs={"k": 1})

    # MultiQueryRetriever ì„¤ì •
    multi_query_retriever = MultiQueryRetriever.from_llm(
        retriever=faiss_retriever,
        llm=llm,
    )

    # ì•™ìƒë¸” retrieverë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    ensemble_retriever = EnsembleRetriever(
        retrievers=[bm25_retriever, multi_query_retriever],
        weights=[0.7, 0.3],
    )
    # Retriever ìƒì„±
    retriever = ensemble_retriever

    # GraphState í´ë˜ìŠ¤ ì •ì˜
    class GraphState(TypedDict):
        question: str       # ì§ˆë¬¸
        context: str        # ë¬¸ì„œì˜ ê²€ìƒ‰ ê²°ê³¼
        answer: str         # ë‹µë³€
        relevance: str      # ë‹µë³€ì˜ ë¬¸ì„œì— ëŒ€í•œ ê´€ë ¨ì„±
        recursion_count: int  # ì¬ê·€ íšŸìˆ˜

    # ë„êµ¬ í•¨ìˆ˜ ì •ì˜
    def json_search_tool(input_text):
        docs = retriever.get_relevant_documents(input_text)
        if docs:
            summaries = [doc.page_content for doc in docs]
            return '\n\n'.join(summaries)
        else:
            return "í•´ë‹¹í•˜ëŠ” ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    #ë„êµ¬ ì„¤ëª… 
    json_search_tool = Tool(
        name="JSONSearch",
        func=json_search_tool,
        description=(
            "Use this tool to search for information about a specific person or issue from the JSON document. "
            "Provides a summary of fields such as conference number, meeting name, generation number, committee name, agenda, law, Q&A type, context, learning context, context summary, questioner, and question. "
            "If a specific person or issue is searched, first prioritize gathering information for exact matches of the search term, and then proceed to work on approximate matches."
        )
    )

    # ë„êµ¬ ëª©ë¡ ìƒì„±
    tools = [json_search_tool]

    # ë©”ëª¨ë¦¬ ì„¤ì •
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    # ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
    agent = initialize_agent(
        tools,
        llm,
        agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,
        verbose=True,
        memory=memory,
    )

    #ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸

    agent.agent.llm_chain.prompt.messages[0].prompt.template = (
        "You are an AI assistant that helps users find information about individuals or issues from the National Assembly JSON document. "
        "When a user asks about a specific person or issue, you should use the JSONSearch tool to find meeting information. "
        "If the user inputs a question in Korean, translate it into English before proceeding with the search. "
        "For queries related to specific policies or legislative issues, summarize the discussions in chronological order and by issue. "
        "Provide a summarized response including fields such as conference number, meeting name, generation number, committee name, agenda, law, Q&A type, context, learning context, context summary, questioner, and question. "
        "Do not mention the use of tools; just provide the necessary information."
        "Be sure to answer in Korean. " 
    )
    # ë¬¸ì„œ ê²€ìƒ‰ì„ ìœ„í•œ í•¨ìˆ˜ ì •ì˜
    def retrieve_document(state: GraphState) -> GraphState:
        # ì¬ê·€ íšŸìˆ˜ ì¦ê°€ ë˜ëŠ” ì´ˆê¸°í™”
        recursion_count = state.get('recursion_count', 0) + 1
        # ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë”°ë¼ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
        retrieved_docs = retriever.get_relevant_documents(state['question'])
        context = '\n\n'.join([doc.page_content for doc in retrieved_docs])
        # ê¸°ì¡´ì˜ 'context'ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        state.update({
            'context': context,
            'recursion_count': recursion_count
        })
        return state

    # ì—ì´ì „íŠ¸ì˜ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ ì •ì˜
    def llm_answer(state: GraphState) -> GraphState:
        # ì—ì´ì „íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
        answer = agent.run(input=state['question'])
        
        # ì¶œë ¥ ê¸€ì ìˆ˜ ì œí•œ
        max_length = 20000
        if len(answer) > max_length:
            answer = answer[:max_length] + '...'  # ìë¥´ê³  '...' ì¶”ê°€
        
        state.update({'answer': answer})
        return state

    # ê·¸ë˜í”„ ì›Œí¬í”Œë¡œìš° ì •ì˜
    workflow = StateGraph(GraphState)

    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("retrieve", retrieve_document)
    workflow.add_node("llm_answer", llm_answer)

    # ì—£ì§€ ì—°ê²°
    workflow.add_edge("retrieve", "llm_answer")

    # ì‹œì‘ì  ì„¤ì •
    workflow.set_entry_point("retrieve")

    # ë©”ëª¨ë¦¬ ì €ì¥ì†Œ ì„¤ì •
    memory_saver = MemorySaver()

    # ì›Œí¬í”Œë¡œìš° ì»´íŒŒì¼
    app = workflow.compile(checkpointer=memory_saver)

    # ì—ì´ì „íŠ¸ì™€ ëŒ€í™”í•˜ëŠ” í•¨ìˆ˜ ìˆ˜ì •
    def chat_with_agent(user_input):
        # ì´ˆê¸° ìƒíƒœ ì„¤ì •
        state = GraphState(
            question=user_input,
            context='',
            answer='',
            recursion_count=0
        )
        config = RunnableConfig(recursion_limit=20, configurable={"thread_id": "SELF-RAG"})
        output = app.invoke(state, config=config)
        recursion_count = output.get('recursion_count', 0)
        
        # ìµœì¢… ë‹µë³€ì— ì¬ê·€ íšŸìˆ˜ë§Œ í¬í•¨
        final_answer = output.get('answer', '')
        # final_answer += f"\n\nì´ ì¬ê·€ íšŸìˆ˜: {recursion_count}"
        
        return final_answer

    # ëŒ€í™” ë‚´ìš©ì„ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜
    def print_messages():
        """ëŒ€í™” ë‚´ìš©ì„ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜"""
        for msg in st.session_state["messages"]:
            st.chat_message(msg['role']).write(msg['content'])


    # Streamlit ë©”ì¸ ì½”ë“œ
    st.title("ì•ˆë…•í•˜ì„¸ìš”! êµ­íšŒ íšŒì˜ë¡ ê²€ìƒ‰AIì…ë‹ˆë‹¤!")  # ì‹œì‘ íƒ€ì´í‹€


    # ì„¸ì…˜ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state["messages"] = [] 

    if "session_history" not in st.session_state:
        st.session_state["session_history"] = {}  # ì„¸ì…˜ ê¸°ë¡ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬
        
    # ì„¸ì…˜ ID ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
    if "session_ids" not in st.session_state:
        st.session_state["session_ids"] = []  # ì„¸ì…˜ ID ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”

    with st.sidebar:
    # ì„¸ì…˜ ID ì…ë ¥
        session_id = st.text_input("ê¸°ë¡í•  ID ì…ë ¥", placeholder="IDë¥¼ ì…ë ¥í•˜ì„¸ìš”")

    # ì„¸ì…˜ ì €ì¥ ë²„íŠ¼
        if st.button("ëŒ€í™”ê¸°ë¡ ì €ì¥"):
            # ì„¸ì…˜ IDê°€ ì…ë ¥ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if session_id:
                if session_id not in st.session_state.session_ids:  # ì¤‘ë³µ ì„¸ì…˜ ID í™•ì¸
                    st.session_state.session_history[session_id] = st.session_state.messages
                    st.session_state["messages"] = []
                    st.session_state.session_ids.append(session_id)  # ì„¸ì…˜ ID ì¶”ê°€
                    st.success(f"ì„¸ì…˜ '{session_id}'ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                else:
                    st.warning("ì´ë¯¸ ì¡´ì¬í•˜ëŠ” IDì…ë‹ˆë‹¤.")
            else:
                st.warning("IDë¥¼ ì…ë ¥í•˜ì„¸ìš”.")

        # ë¶ˆëŸ¬ì˜¬ ì„¸ì…˜ ì„ íƒ
        if st.session_state.session_ids:  # ì„¸ì…˜ IDê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ë“œë¡­ë‹¤ìš´ í‘œì‹œ
            selected_session_id = st.selectbox("ë¶ˆëŸ¬ì˜¬ ID ì„ íƒ", options=st.session_state.session_ids)
            # ì„¸ì…˜ ë¶ˆëŸ¬ì˜¤ê¸° ë²„íŠ¼
            if st.button("ê¸°ë¡ ë¶ˆëŸ¬ì˜¤ê¸°"):
                # if selected_session_id in st.session_state.session_history.keys():
                st.session_state.messages = st.session_state.session_history[selected_session_id]
                st.success(f"ëŒ€í™”ê¸°ë¡ì„ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")  
                    
        else:
            st.info("ì €ì¥ëœ ëŒ€í™”ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")

        # ì„¸ì…˜ ì´ˆê¸°í™” ë° ì„ì‹œì €ì¥
        if st.button("ìƒˆë¡œìš´ ëŒ€í™” ìƒì„±"):
            st.session_state.session_ids.append("ì´ì „ ëŒ€í™”ê¸°ë¡")
            st.session_state.session_history["ì´ì „ ëŒ€í™”ê¸°ë¡"] = st.session_state.messages
            st.session_state["messages"] = []

        
        if session_id :
                session_id = selected_session_id
        else :
            session_id = "default_session"


    # ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ì—ì´ì „íŠ¸ì— ì „ë‹¬
    if st.session_state.messages:
        previous_messages = [{"role": msg['role'], "content": msg['content']} for msg in st.session_state.messages]


    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    user_input = st.chat_input('ì§ˆë¬¸ì´ ë¬´ì—‡ì¸ê°€ìš”?')

    if user_input:
        # ì„¸ì…˜ IDë¥¼ ì„¤ì •
        if st.session_state.messages:
            response = chat_with_agent(user_input + "\n\nPrevious Messages: " + str(previous_messages))
        else:
            response = chat_with_agent(user_input)

        # ë©”ì‹œì§€ë¥¼ ì„¸ì…˜ì— ì¶”ê°€
        st.session_state["messages"].append({"role": "user", "content": user_input})
        st.session_state["messages"].append({"role": "assistant", "content": response})

    # ëŒ€í™” ë‚´ìš© ì¶œë ¥
    print_messages()

else:
    st.warning("API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
